<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>炼金</title>
    <script type="text/javascript" src="/assets/js/prism.js"></script>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <script type="text/javascript" src="/assets/js/jquery-3.6.0.js"></script>
    <script src="/assets/js/scrollnav.min.umd.js"></script>
  <script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
  <body>
    <div class="wrapper">
      <div class="clearfloat">
        <nav class="navbar">
    <!-- <a href="/" >
        Home
    </a>
    <a href="/about.html" >
        About
    </a> -->
    
        <!-- <a href="https://github.com/Zhniing"  target="_self"> -->
        <a href="https://github.com/Zhniing" target="_blank">
            Github
        </a>
    
        <!-- <a href="/about.html"  target="_self"> -->
        <a href="/about.html" target="_self">
            About
        </a>
    
        <!-- <a href="/games/hiker"  target="_self"> -->
        <a href="/games/hiker" target="_self">
            Game
        </a>
    
        <!-- <a href="/"  target="_self"> -->
        <a href="/" target="_self">
            Home
        </a>
    
</nav>
      </div>
      <base target="_blank"> <!-- 仅post页面的a标签为_blank -->

<h1>炼金</h1>
<!-- <p>Posted on 13 Nov 2020, Last updated: </p> -->
<!-- <p>发表于 2020 年 11 月 13 日 | 更新于 </p> -->
<p>发布于 2020 年 11 月 13 日  | 更新于 2021 年 09 月 07 日 </p>

<div class="post_content">
    <h1 id="为什么用深度学习">为什么用深度学习</h1>

<p>深度学习可以自己<strong>学会</strong>如何提取有用的特征，相反，机器学习则十分依赖<a href="https://zhuanlan.zhihu.com/p/111296130">特征工程</a>。</p>

<p>上手门槛低。</p>

<h2 id="常见术语">常见术语</h2>

<ul>
  <li>
    <p>ResNet层数</p>

    <p>resnet18 = 16 + 2</p>

    <p>resnet34 = 16 <strong>* 2</strong> + 2</p>

    <p>resnet50 = 改为bottleneck，总共增加了16层</p>

    <p>resnet101 = 增加第4区的块数</p>

    <p>resnet152 = 增加第3、4区的块数</p>
  </li>
  <li>
    <p>Backbone</p>

    <p>CV领域中，一般要先<strong>提取图像的特征</strong>，再利用这些特征去干一些事情（分类，识别，分割等）。因此，提取特征的部分被称作网络的主干（骨干，backbone）。常见的backbone有ResNet、VGG等。</p>
  </li>
  <li>
    <p>head</p>

    <p>利用之前的特征，得到网络最后的输出（做出预测），一般位于网络的最后。</p>
  </li>
  <li>
    <p>neck</p>

    <p>一般位于Backbone和head之间，为了更好地利用Backbone提取的特征。</p>
  </li>
  <li>
    <p>Embedding</p>

    <p>嵌入：将<strong>原始数据</strong>转变为<strong>向量/特征</strong>的过程（提取特征的过程）。<a href="https://www.zhihu.com/question/38002635/answer/1382442522">reference</a></p>
  </li>
</ul>

<blockquote>
  <p><a href="https://blog.csdn.net/t20134297/article/details/105745566">reference</a></p>
</blockquote>

<h2 id="解决过拟合">解决过拟合</h2>

<ol>
  <li>权值衰减(weight decay): Adam优化器自带的参数</li>
  <li>学习率衰减：自己手动设置规则，或pytorch提供现成的方法</li>
  <li>Dropout</li>
  <li>Batch Normalization: 与Relu一起使用, 防止Dead Relu, 也能加速训练?</li>
  <li>增加训练集的样本数</li>
</ol>

<h3 id="实验结果">实验结果</h3>

<p>之前实验发生过拟合的原因是：定义模型结构的时候，复用了编码和解码路径上的（等通道）卷积模块，导致这些模块变成了<strong>权值共享</strong>（或者说是强行使编码和解码路径上部分模块的权值相同）。修改模型代码后过拟合问题消失。</p>

<p>修改前模型参数量：6238532，修改后：8563652，证实了前面的分析</p>

<p>代码：<a href="https://github.com/Zhniing/U-net/blob/3a2b94dad09782bde7afb7f0d758b0b2cde70a33/network.py#L6">修改前的Unet</a>和<a href="https://github.com/Zhniing/U-net/blob/a106b23a063f132e9bc06c301c79bceb820128fb/network.py#L74">修改后的Unet2</a></p>

<h3 id="调整学习率">调整学习率</h3>

<ul>
  <li>
    <p>自己手动设置规则：最后几轮进行衰减</p>

    <p>设定一个衰减轮数n，最后n轮每轮使lr减掉初始lr的n分之1，持续n轮，最后减为0</p>

    <p>$$
lr=lr-\frac{lr_{begin}}{n}
$$</p>

    <p>实验结果：在lr开始衰减时，可以看到训练集有明显提升（mean Dice提升2%左右），但<strong>验证集几乎没有影响</strong></p>
  </li>
</ul>

<hr>

<ul>
  <li>
    <p>Pytorch提供的现成的方法：随着迭代次数增加，学习率自动衰减</p>

    <ol>
      <li>
        <p>指数衰减 ExponentialLR
$$
lr = lr_{base} * \gamma ^ {epoch}
\tag {1}
$$</p>
      </li>
      <li>
        <p>等步长衰减 StepLR</p>

        <p>每隔一定epoch，就衰减为原来的γ（gamma）倍（0&lt;γ&lt;1）</p>

        <p>比如step_size=30，则在epoch为30、60、90时进行调整</p>

        <p>$$
lr = lr_{base} * \gamma ^ {\lfloor \frac{epoch}{stepsize} \rfloor}
$$</p>
      </li>
      <li>
        <p>多步长衰减 MultiStepLR</p>

        <p>达到milestones=[200, 300, 320, 340, 400]指定的步数时，学习率衰减为γ倍</p>
      </li>
    </ol>

    <hr>

    <p><strong>小结</strong>：其实前面三种方法本质上都是按epoch进行衰减，只是发生衰减的步数不同</p>

    <ul>
      <li>指数衰减就是每个epoch都衰减一次</li>
      <li>等步长就是间隔固定的步长后衰减一次</li>
      <li>多步长就是自定义在第几个epoch进行衰减</li>
    </ul>

    <hr>

    <ol>
      <li>
        <p>余弦退火衰减 CosineAnnealingLR</p>

        <p>学习率周期变化，而不是衰减</p>
      </li>
      <li>
        <p>自适应调整学习率 ReduceLROnPlateau</p>

        <blockquote>
          <p>当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略</p>
        </blockquote>
      </li>
      <li>
        <p>自定义调整学习率 LambdaLR</p>

        <blockquote>
          <p>为不同参数组设定不同学习率调整策略，fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略</p>
        </blockquote>
      </li>
    </ol>
  </li>
  <li>
    <p>在每个epoch结束时调用返回的scheduler对象的step()方法</p>

    <p><code class="language-plaintext highlighter-rouge">scheduler.step()</code></p>
  </li>
  <li>
    <p>查看当前学习率</p>

    <p><code class="language-plaintext highlighter-rouge">scheduler.get_lr()</code></p>

    <p>可以通过查看每个类对get_lr()的实现方法，来找到每个类的学习率调整公式</p>
  </li>
  <li>
    <p>示例</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="nc">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
	<span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
	<span class="c1"># train(...)
</span>	<span class="c1"># validate(...)
</span></code></pre></div>    </div>
    <blockquote>
      <p><a href="https://blog.csdn.net/shanglianlm/article/details/85143614">参考博客</a></p>

      <p><a href="https://pytorch.org/docs/1.2.0/optim.html#how-to-adjust-learning-rate">官方文档</a></p>
    </blockquote>
  </li>
</ul>

<h3 id="dropout">Dropout</h3>

<blockquote>
  <p>一般适合于全连接层，卷积层由于参数不多，所以不是很需要Dropout，加上也对模型泛化能力没有太大影响。</p>
</blockquote>

<p>干的事情：</p>

<ul>
  <li>Pytorch的Dropout设置的p是失活率（drop rate）</li>
  <li>不是按失活比例进行失活，而是对每个神经元按失活率进行失活（对每个输出点按失活率进行失活）。因此，<strong>实际失活的数量不一定等于失活率的比例</strong>
</li>
</ul>

<p>实验：</p>

<ul>
  <li>Pytorch的Dropout分为：
    <ul>
      <li>Dropout</li>
      <li>Dropout2d</li>
      <li>Dropout3d</li>
    </ul>
  </li>
  <li>不加dropout时，训练集效果最好，p越高，训练集效果越差</li>
  <li>在p（失活率）为0.1~0.8的实验中，p=0.1的训练、验证结果最好</li>
  <li>p为0.6时，训练集性能下降明显（验证集也一样），p为0.7和0.8时，就几乎学不动了，训练和验证的Dice很早就收敛到0.5左右</li>
</ul>

<p>总结：</p>

<ul>
  <li>drop rate越高训练集上的效果越差，感觉像是强行降低在训练集上的拟合能力来提高泛化能力。。。</li>
  <li>确实能够抑制一点过拟合，但是dice依然抖动严重，可能还需要加上学习率衰减。</li>
</ul>

<h1 id="权值初始化">权值初始化</h1>

<ol>
  <li>kaiming初始化</li>
</ol>

<h1 id="数据归一化标准化">数据归一化/标准化</h1>

<ol>
  <li>数据点(x, y)的归一化：为了降低数据(x和y)之间的相关性，去除冗余信息，如：<strong>正相关</strong>变为<strong>不相关</strong>
</li>
  <li>图像像素值归一化：为了将像素值约束到一个<strong>合理范围</strong>内，便于后续神经网络的学习
    <ul>
      <li>BN：将数据归一化到<strong>均值0，方差1</strong>。通常卷积后数据的方差较小，即数据的<strong>分布比较集中</strong>，差异较小，不便于后序网络的学习</li>
    </ul>
  </li>
</ol>

<h1 id="下采样">下采样</h1>

<p>仅用<strong>卷积</strong>来下采样的话（不做BN归一化），深层特征图上的值的<strong>绝对值会快速趋于0</strong>。</p>

<p>解决方法：卷积后进行归一化<strong>BN</strong></p>

<h1 id="性能">性能</h1>

<ul>
  <li>深层参数量大，但显存消耗较小</li>
  <li>浅层参数量小，但显存消耗较大</li>
</ul>

</div>
    </div>
    <script>
      const ct = document.querySelector('.post_content');  //".post-content"指向文章内容所在的div，需根据实际情况修改
      scrollnav.init(ct, {
          debug: false,
          easingStyle: 'linear',
          //section为一级目录，subsection为二级目录
          sections: ($('.post_content > h1').length>0) ? 'h1' : 'h2',
          subSections: ($('.post_content > h1').length>0) ? 'h2' : 'h3',
      });
    </script>
    <script type="text/javascript">
      $('pre').addClass("line-numbers").css("white-space", "pre-wrap");
    </script>
  </body>
</html>
